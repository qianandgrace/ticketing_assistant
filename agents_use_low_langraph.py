import logging
import asyncio
import sys
from typing import TypedDict
import uuid

from langgraph.checkpoint.memory import InMemorySaver
from langgraph.prebuilt import create_react_agent
from langchain_core.messages import SystemMessage, HumanMessage
from typing import TypedDict
from langchain_core.runnables import RunnableConfig
from langgraph.types import Command
from langgraph.store.postgres import AsyncPostgresStore
from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver

from llm import get_llm
from utils import parse_messages, save_graph_visualization, pre_model_hook, add_human_in_the_loop
from tools import get_tools

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(name)s | %(message)s"
)
logger = logging.getLogger("ticket_assistant")
if sys.platform.startswith("win"):
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

class UserConfig(TypedDict):
        user_id: str

llm = get_llm("openai")
DB_URI= "postgresql://gq210:123456@localhost:5432/postgres?options=-csearch_path%3Dticket_assistant_memory"

async def non_streamble_ivoke(agent, config: RunnableConfig, messages: str, debug: bool = False):
    """
    Docstring for non_streamble_ivoke
    """
    agent_response = await agent.ainvoke({"messages": [HumanMessage(content=messages)]}, config)
    # # å°†è¿”å›çš„messagesè¿›è¡Œæ ¼å¼åŒ–è¾“å‡º
    if debug:
        parse_messages(agent_response['messages'])
    agent_response_content = agent_response["messages"][-1].content
    logger.info(f"initial_agent_response:{agent_response_content}")
    # (1)æ¨¡æ‹Ÿäººç±»åé¦ˆï¼šæµ‹è¯•3ç§åé¦ˆæ–¹å¼
    agent_response = await agent.ainvoke(
        Command(resume=[{"type": "accept"}]),
        # Command(resume=[{"type": "edit", "args": {"args": {'train_number': 'G805'}}}]),
        # Command(resume=[{"type": "reject", "args": "æˆ‘ä¸æƒ³æŸ¥è¯¢äº†"}]),
        config
    )
    # å°†è¿”å›çš„messagesè¿›è¡Œæ ¼å¼åŒ–è¾“å‡º
    if debug:
        parse_messages(agent_response['messages'])
    agent_response_content = agent_response["messages"][-1].content
    logger.info(f"final_agent_response:{agent_response_content}")


async def stream_ivoke(agent, config: RunnableConfig, message: str, debug: bool = False):
    """
    Docstring for stream_ivoke
    
    :param agent: Description
    :param config: Description
    :type config: RunnableConfig
    :param message: Description
    :type message: str
    :param debug: Description
    :type debug: bool
    """ 
    async for message_chunk, metadata in agent.astream(
            input={"messages": [HumanMessage(content=message)]},
            config=config,
            stream_mode="messages"
    ):
        # æµ‹è¯•åŸå§‹è¾“å‡º
        if debug:
            logger.info(f"Message Chunk: {message_chunk}")
            logger.info(f"Metadata: {metadata}")    
    
        # è·³è¿‡å·¥å…·è¾“å‡º
        if metadata["langgraph_node"]=="tools":
            continue
    
        # è¾“å‡ºæœ€ç»ˆç»“æœ
        if message_chunk.content:
            print(message_chunk.content, end="|", flush=True)
    
    # æ¨¡æ‹Ÿäººç±»åé¦ˆï¼šæµ‹è¯•3ç§åé¦ˆæ–¹å¼
    async for message_chunk, metadata in agent.astream(
        Command(resume=[{"type": "accept"}]),
        # Command(resume=[{"type": "edit", "args": {"args": {'location': '120.619585,31.299379'}}}]),
        # Command(resume=[{"type": "response", "args": "æˆ‘ä¸æƒ³æŸ¥è¯¢äº†"}]),
        config,
        stream_mode="messages"
    ):
        # æµ‹è¯•åŸå§‹è¾“å‡º
        if debug:
            logger.info(f"Message Chunk: {message_chunk}")
            logger.info(f"Metadata: {metadata}")
    
        # è·³è¿‡å·¥å…·è¾“å‡º
        if metadata["langgraph_node"]=="tools":
            continue
        # è¾“å‡ºæœ€ç»ˆç»“æœ
        if message_chunk.content:
            print(message_chunk.content, end="", flush=True)
    
async def load_long_term_memory(store, user_id: str) -> str:
    namespace = ("memories", user_id)
    memories = await store.asearch(namespace, query="")

    if not memories:
        logger.info("ğŸ“¦ é•¿æœŸè®°å¿†ï¼šæ— ")
        return "æ— é•¿æœŸè®°å¿†ä¿¡æ¯"

    info = " ".join([m.value["data"] for m in memories])
    logger.info("ğŸ“¦ é•¿æœŸè®°å¿†æ£€ç´¢ç»“æœ: %s", info)
    return info

# å®šä¹‰å¹¶è¿è¡Œagent
async def run_agent(save_node=False, store_memory: bool = False):
    # ä»MCP Serverä¸­è·å–å¯æä¾›ä½¿ç”¨çš„å…¨éƒ¨å·¥å…·
    # MCP Client èƒ½å¤ŸåŠ¨æ€æ„ŸçŸ¥å·¥å…·çš„å˜åŒ–
    async with (
        AsyncPostgresSaver.from_conn_string(DB_URI) as checkpointer,
        AsyncPostgresStore.from_conn_string(DB_URI) as store,
    ):
        await checkpointer.setup()
        await store.setup()
        all_tools = await get_tools()
        # 12306å·¥å…·ä½¿ç”¨è¿™ç§æ–¹å¼å­˜åœ¨bugï¼Œæš‚æ—¶ä¸æ¸…æ¥šåŸå› 
        # tools = [await add_human_in_the_loop(all_tools[6])]
        add_human_tools = [await add_human_in_the_loop(index) for index in all_tools[8:]] # type: ignore
        tools = all_tools[:8] + add_human_tools

        # å®šä¹‰ç³»ç»Ÿæ¶ˆæ¯
        system_message = SystemMessage(content=(
            "ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ã€‚"
        ))
        # åˆ›å»ºReActé£æ ¼çš„agent
        agent = create_react_agent(
            model=llm,
            tools=tools,
            prompt=system_message,
            checkpointer=InMemorySaver(), 
            # è¿™æ ·å†™ä¼šæŠ¥é”™
            # checkpointer=checkpointer,
            store=store,
            pre_model_hook=pre_model_hook
        )

        # å°†å®šä¹‰çš„agentçš„graphè¿›è¡Œå¯è§†åŒ–è¾“å‡ºä¿å­˜è‡³æœ¬åœ°
        if save_node:
            save_graph_visualization(agent)
    
        # å®šä¹‰ç”¨æˆ·é…ç½®å’Œçº¿ç¨‹ID
        user_config = UserConfig(user_id = "1")
        config: RunnableConfig = {
            "configurable": {
            "thread_id": "5",
            **user_config
            }
        }
        if store_memory:
            # è‡ªå®šä¹‰å­˜å‚¨é€»è¾‘ å¯¹ç”¨æˆ·è¾“å…¥è¿›è¡Œå¤„ç†ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦å­˜å‚¨é•¿æœŸè®°å¿†
            namespace = ("memories", config["configurable"]["user_id"])
            memory1 = "æˆ‘çš„åå­—å«gq"
            await store.aput(namespace, str(uuid.uuid4()), {"data": memory1})
            memory2 = "æˆ‘çš„è®¢ç¥¨åå¥½æ˜¯åªå®šä»·æ ¼æœ€ä½çš„è½¦æ¬¡"
            await store.aput(namespace, str(uuid.uuid4()), {"data": memory2})
        info = await load_long_term_memory(store, user_config["user_id"])
        logger.info(f"é•¿æœŸè®°å¿†ä¿¡æ¯åŠ è½½å®Œæˆ: {info}")

        # # 1ã€éæµå¼å¤„ç†æŸ¥è¯¢
        """
        æ¡ˆä¾‹ï¼šé¢„å®š1631æ¬¡åˆ—è½¦ï¼Œ è°ƒç”¨å·¥å…·æŸ¥è¯¢ä¸‹ä¸Šæµ·çš„å¤©æ°” è°ƒç”¨å·¥å…·æŸ¥è¯¢åŒ—äº¬åˆ°ä¸Šæµ·çš„é«˜é“ç­æ¬¡
        """
        message = "é¢„å®šæ˜å¤©åŒ—äº¬åˆ°ä¸Šæµ·çš„é«˜é“ç­æ¬¡"
        message += info
        await non_streamble_ivoke(agent, config, message, debug=True)
        return


        # 2ã€æµå¼å¤„ç†æŸ¥è¯¢
        # await stream_ivoke(agent, config, "æŸ¥è¯¢ä¸Šæµ·çš„å¤©æ°”", debug=False)
        # return


if __name__ == "__main__":
    asyncio.run(run_agent())


